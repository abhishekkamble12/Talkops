=====================================================
   VOICE AGENT INTEGRATION SOLUTION
=====================================================

Your voice agent (vaoice_agent.ts) needs proper integration.
Here's the complete step-by-step solution:

=====================================================
   OPTION 1: SIMPLE INTEGRATION (Recommended)
=====================================================

The easiest approach is to add TTS (Text-to-Speech) output
to your EXISTING flow. When agents respond with text,
convert that text to speech.

FILES TO CREATE/MODIFY:
-----------------------

1. RENAME: vaoice_agent.ts → Voice_output.step.ts

2. CREATE: Voice_input.step.ts (API endpoint for voice input)

3. MODIFY: Havoc_response.step.ts and Hulk_response.step.ts
   to emit voice responses

=====================================================
   ARCHITECTURE OVERVIEW
=====================================================

CURRENT FLOW:
  POST /api/order/query (text)
       ↓
  Agents.step.ts (routes to havoc/hulk)
       ↓
  Agent_havoc.step.ts OR Agents_hulk.step.ts
       ↓
  Response saved to state
       ↓
  GET /api/order/response/:requestId (text response)


NEW FLOW WITH VOICE:
  POST /api/voice/query (audio file OR text)
       ↓
  Voice_input.step.ts (converts audio → text using STT)
       ↓
  Emits to 'google.analyzequeryRequest' (same as text flow)
       ↓
  [Same agent flow as before]
       ↓
  Response handlers emit to 'voice.synthesize'
       ↓
  Voice_output.step.ts (converts text → audio using TTS)
       ↓
  Audio saved to state/file
       ↓
  GET /api/voice/response/:requestId (returns audio URL)

=====================================================
   STEP 1: INSTALL DEPENDENCIES
=====================================================

Run this command in your hackathon_project folder:

npm install groq-sdk

Add to your .env file:
GROQ_API_KEY=your_groq_api_key_here

=====================================================
   STEP 2: CREATE Voice_input.step.ts
=====================================================

Create file: src/steps/Voice_input.step.ts

This API endpoint receives audio/text and routes it
to your existing agent flow.

```typescript
import { ApiRouteConfig, ApiRouteHandler, ApiResponse } from 'motia'
import { z } from 'zod'
import { randomUUID } from 'crypto'
import Groq from 'groq-sdk'

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY })

const userInfoSchema = z.object({
  email: z.string().optional(),
  customerId: z.string().optional(),
  orderId: z.string().optional(),
  trackingNumber: z.string().optional(),
  phone: z.string().optional(),
  name: z.string().optional(),
})

const bodySchema = z.object({
  // Either text or audioBase64 is required
  text: z.string().optional(),
  audioBase64: z.string().optional(), // Base64 encoded audio
  userInfo: userInfoSchema.optional(),
  enableVoiceResponse: z.boolean().optional().default(true),
})

type BodyType = z.infer<typeof bodySchema>
type UserInfo = z.infer<typeof userInfoSchema>

type EmitData = {
  topic: 'google.analyzequeryRequest'
  data: { text: string; userInfo?: UserInfo; requestId: string; enableVoiceResponse: boolean }
}

export const config: ApiRouteConfig = {
  type: 'api',
  name: 'voiceInput',
  description: 'Receives voice/text input and routes to agents',
  path: '/api/voice/query',
  method: 'POST',
  emits: ['google.analyzequeryRequest'],
  bodySchema: bodySchema as any,
  flows: ['customer-support'],
}

export const handler: ApiRouteHandler<BodyType, ApiResponse, EmitData> = async (req, { emit, logger, state }) => {
  const { text, audioBase64, userInfo, enableVoiceResponse } = req.body
  const requestId = randomUUID()

  let queryText = text || ''

  // If audio is provided, transcribe it using Groq Whisper
  if (audioBase64 && !text) {
    try {
      const audioBuffer = Buffer.from(audioBase64, 'base64')
      // Save temporarily for Groq API
      const fs = await import('fs')
      const tempPath = `/tmp/audio_${requestId}.wav`
      await fs.promises.writeFile(tempPath, audioBuffer)

      const transcription = await groq.audio.transcriptions.create({
        file: fs.createReadStream(tempPath),
        model: 'whisper-large-v3',
        response_format: 'text',
      })

      queryText = transcription.text || transcription
      
      // Clean up temp file
      await fs.promises.unlink(tempPath).catch(() => {})
      
      logger.info('[VoiceInput] Transcribed audio', { transcription: queryText })
    } catch (error: any) {
      logger.error('[VoiceInput] Transcription failed', { error: error?.message })
      return {
        status: 400,
        body: { error: 'Failed to transcribe audio', requestId },
      }
    }
  }

  if (!queryText) {
    return {
      status: 400,
      body: { error: 'Either text or audioBase64 is required', requestId },
    }
  }

  logger.info('[VoiceInput] Processing query', { text: queryText, requestId })

  // Initialize state
  await state.set('responses', requestId, {
    status: 'processing',
    requestId,
    query: queryText,
    enableVoiceResponse: enableVoiceResponse ?? true,
    createdAt: new Date().toISOString(),
  })

  // Route to existing agent flow
  await emit({
    topic: 'google.analyzequeryRequest',
    data: { 
      text: queryText, 
      userInfo, 
      requestId,
      enableVoiceResponse: enableVoiceResponse ?? true,
    },
  })

  return {
    status: 200,
    body: {
      status: 'Accepted',
      message: 'Your voice query is being processed',
      requestId,
      transcribedText: queryText,
    },
  }
}
```

=====================================================
   STEP 3: CREATE Voice_output.step.ts
=====================================================

Rename vaoice_agent.ts to Voice_output.step.ts and update:

```typescript
import { EventConfig, EventHandler } from 'motia'
import { z } from 'zod'
import Groq from 'groq-sdk'
import fs from 'fs'
import path from 'path'

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY })

const inputSchema = z.object({
  text: z.string(),
  requestId: z.string(),
})

type InputType = z.infer<typeof inputSchema>

export const config: EventConfig = {
  type: 'event',
  name: 'VoiceOutput',
  description: 'Converts text response to speech using Groq TTS',
  subscribes: ['voice.synthesize'],
  emits: [],
  input: inputSchema as any,
  flows: ['customer-support'],
}

export const handler: EventHandler<InputType, never> = async (input, { logger, state }) => {
  const { text, requestId } = input

  logger.info('[VoiceOutput] Synthesizing speech', { requestId, textLength: text.length })

  try {
    const response = await groq.audio.speech.create({
      model: 'playai-tts',
      voice: 'Fritz-PlayAI',
      input: text,
      response_format: 'wav',
    })

    const buffer = Buffer.from(await response.arrayBuffer())
    
    // Save audio file
    const audioDir = path.join(process.cwd(), 'public', 'audio')
    await fs.promises.mkdir(audioDir, { recursive: true })
    
    const audioFileName = `response_${requestId}.wav`
    const audioFilePath = path.join(audioDir, audioFileName)
    
    await fs.promises.writeFile(audioFilePath, buffer)

    // Update state with audio URL
    const currentState = await state.get('responses', requestId) as any
    await state.set('responses', requestId, {
      ...currentState,
      audioUrl: `/audio/${audioFileName}`,
      audioReady: true,
    })

    logger.info('[VoiceOutput] Speech synthesized', { requestId, audioUrl: `/audio/${audioFileName}` })
  } catch (error: any) {
    logger.error('[VoiceOutput] TTS failed', { error: error?.message, requestId })
  }
}
```

=====================================================
   STEP 4: MODIFY RESPONSE HANDLERS
=====================================================

Update Havoc_response.step.ts to emit voice synthesis:

Add to emits: ['voice.synthesize']

Add at end of handler:
```typescript
// Emit for voice synthesis if enabled
const responseState = await state.get('responses', requestId) as any
if (responseState?.enableVoiceResponse) {
  await emit({
    topic: 'voice.synthesize',
    data: { text: response, requestId },
  })
}
```

Do the same for Hulk_response.step.ts

=====================================================
   STEP 5: UPDATE Agents.step.ts INPUT SCHEMA
=====================================================

Add enableVoiceResponse to the inputSchema:

```typescript
const inputSchema = z.object({
  text: z.string(),
  userInfo: userInfoSchema.optional(),
  requestId: z.string(),
  enableVoiceResponse: z.boolean().optional(),
})
```

Pass it through to agent data.

=====================================================
   STEP 6: CREATE GET Voice Response API
=====================================================

Create file: src/steps/GetVoiceResponse.step.ts

```typescript
import { ApiRouteConfig, ApiRouteHandler, ApiResponse } from 'motia'

export const config: ApiRouteConfig = {
  type: 'api',
  name: 'getVoiceResponse',
  description: 'Retrieves voice response status and audio URL',
  path: '/api/voice/response/:requestId',
  method: 'GET',
  emits: [],
  flows: ['customer-support'],
}

export const handler: ApiRouteHandler<unknown, ApiResponse, never> = async (req, { logger, state }) => {
  const { requestId } = req.pathParams

  const responseData = await state.get('responses', requestId) as any

  if (!responseData) {
    return {
      status: 404,
      body: { error: 'Response not found', requestId },
    }
  }

  return {
    status: 200,
    body: {
      status: responseData.status,
      requestId,
      response: responseData.response,
      audioUrl: responseData.audioUrl,
      audioReady: responseData.audioReady ?? false,
    },
  }
}
```

=====================================================
   USAGE EXAMPLES
=====================================================

1. SEND TEXT (with voice response):
curl -X POST http://localhost:3000/api/voice/query \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Where is my package?",
    "userInfo": {"email": "test@example.com"},
    "enableVoiceResponse": true
  }'

2. SEND AUDIO (Base64 encoded):
curl -X POST http://localhost:3000/api/voice/query \
  -H "Content-Type: application/json" \
  -d '{
    "audioBase64": "UklGRi...(base64 wav data)...",
    "enableVoiceResponse": true
  }'

3. GET RESPONSE:
curl http://localhost:3000/api/voice/response/REQUEST_ID

Response:
{
  "status": "completed",
  "response": "Your package is...",
  "audioUrl": "/audio/response_REQUEST_ID.wav",
  "audioReady": true
}

=====================================================
   FRONTEND INTEGRATION
=====================================================

JavaScript example for recording and sending audio:

```javascript
// Record audio
const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
const mediaRecorder = new MediaRecorder(stream)
const chunks = []

mediaRecorder.ondataavailable = (e) => chunks.push(e.data)
mediaRecorder.onstop = async () => {
  const blob = new Blob(chunks, { type: 'audio/wav' })
  const reader = new FileReader()
  reader.onloadend = async () => {
    const base64 = reader.result.split(',')[1]
    
    // Send to API
    const res = await fetch('/api/voice/query', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ audioBase64: base64 })
    })
    const { requestId } = await res.json()
    
    // Poll for response
    const poll = setInterval(async () => {
      const r = await fetch(`/api/voice/response/${requestId}`)
      const data = await r.json()
      if (data.audioReady) {
        clearInterval(poll)
        // Play audio
        const audio = new Audio(data.audioUrl)
        audio.play()
      }
    }, 1000)
  }
  reader.readAsDataURL(blob)
}

// Start recording
mediaRecorder.start()
setTimeout(() => mediaRecorder.stop(), 5000) // Record for 5 seconds
```

=====================================================
   SUMMARY OF FILES TO CREATE
=====================================================

1. src/steps/Voice_input.step.ts    - API for voice/text input
2. src/steps/Voice_output.step.ts   - TTS synthesis step
3. src/steps/GetVoiceResponse.step.ts - API to get audio response

MODIFY:
4. src/steps/Havoc_response.step.ts - Add voice emit
5. src/steps/Hulk_response.step.ts  - Add voice emit
6. src/steps/Agents.step.ts         - Pass enableVoiceResponse
7. .env                             - Add GROQ_API_KEY

DELETE (or keep as reference):
8. src/steps/vaoice_agent.ts
9. src/steps/voice.ts

=====================================================
   QUICK START (MINIMAL CHANGES)
=====================================================

If you just want TTS output without audio input:

1. Create Voice_output.step.ts (as shown above)
2. Modify response handlers to emit 'voice.synthesize'
3. Add GROQ_API_KEY to .env
4. Responses will include audioUrl when ready

This way you can use the existing text API and just
get voice responses back!

=====================================================

